# Support 7B LRMs

from opencompass.models import (
    TurboMindModelwithChatTemplate,
)

models = sum([v for k, v in locals().items() if k.endswith('_model')], [])

models += [
    # You can comment out the models you don't want to evaluate
    # All models use sampling mode
    dict(
        type=TurboMindModelwithChatTemplate,
        abbr='deepseek-ai--DeepSeek-R1-Distill-Qwen-7B-turbomind',
        path='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B',
        engine_config=dict(session_len=34768, max_batch_size=128, tp=1),
        gen_config=dict(
                        do_sample=True,
                        temperature=0.6,
                        top_p=0.95,
                        max_new_tokens=32768),
        max_seq_len=34768,
        max_out_len=32768,
        batch_size=128,
        run_cfg=dict(num_gpus=1),
    ),
    dict(
        type=TurboMindModelwithChatTemplate,
        abbr='daman1209arora--alpha_0.1_DeepSeek-R1-Distill-Qwen-7B-turbomind',
        path='daman1209arora/alpha_0.1_DeepSeek-R1-Distill-Qwen-7B',
        engine_config=dict(session_len=34768, max_batch_size=128, tp=1),
        gen_config=dict(
                        do_sample=True,
                        temperature=0.6,
                        top_p=0.95,
                        max_new_tokens=32768),
        max_seq_len=34768,
        max_out_len=32768,
        batch_size=128,
        run_cfg=dict(num_gpus=1),
    ),
    dict(
        type=TurboMindModelwithChatTemplate,
        abbr='daman1209arora--alpha_0.2_DeepSeek-R1-Distill-Qwen-7B-turbomind',
        path='daman1209arora/alpha_0.2_DeepSeek-R1-Distill-Qwen-7B',
        engine_config=dict(session_len=34768, max_batch_size=128, tp=1),
        gen_config=dict(
                        do_sample=True,
                        temperature=0.6,
                        top_p=0.95,
                        max_new_tokens=32768),
        max_seq_len=34768,
        max_out_len=32768,
        batch_size=128,
        run_cfg=dict(num_gpus=1),
    ),
        dict(
        type=TurboMindModelwithChatTemplate,
        abbr='deepseek-ai--DeepSeek-R1-Distill-Llama-8B-turbomind',
        path='deepseek-ai/DeepSeek-R1-Distill-Llama-8B',
        engine_config=dict(session_len=34768, max_batch_size=128, tp=1),
        gen_config=dict(
                        do_sample=True,
                        temperature=0.6,
                        top_p=0.95,
                        max_new_tokens=32768),
        max_seq_len=34768,
        max_out_len=32768,
        batch_size=128,
        run_cfg=dict(num_gpus=1),
    ),
    dict(
        type=TurboMindModelwithChatTemplate,
        abbr='meta-llama--Llama-3.1-8B-Instruct-turbomind',
        path='meta-llama/Llama-3.1-8B-Instruct',
        engine_config=dict(session_len=34768, max_batch_size=128, tp=1),
        gen_config=dict(
                        do_sample=False,
                        temperature=0.0,
                        top_p=1.0,
                        max_new_tokens=32768),
        max_seq_len=34768,
        max_out_len=32768,
        batch_size=128,
        run_cfg=dict(num_gpus=1),
    ),
    dict(
        type=TurboMindModelwithChatTemplate,
        abbr='inclusionAI--AReaL-boba-RL-7B-turbomind',
        path='inclusionAI/AReaL-boba-RL-7B',
        engine_config=dict(session_len=34768, max_batch_size=128, tp=1),
        gen_config=dict(
                        do_sample=True,
                        temperature=0.6,
                        top_p=0.95,
                        max_new_tokens=32768),
        max_seq_len=34768,
        max_out_len=32768,
        batch_size=128,
        run_cfg=dict(num_gpus=1),
    ),
    dict(
        type=TurboMindModelwithChatTemplate,
        abbr='qihoo360--Light-R1-7B-DS-turbomind',
        path='qihoo360/Light-R1-7B-DS',
        engine_config=dict(session_len=34768, max_batch_size=128, tp=1),
        gen_config=dict(
                        do_sample=True,
                        temperature=0.6,
                        top_p=0.95,
                        max_new_tokens=32768),
        max_seq_len=34768,
        max_out_len=32768,
        batch_size=128,
        run_cfg=dict(num_gpus=1),
    ),
    dict(
        type=TurboMindModelwithChatTemplate,
        abbr='open-r1--OpenR1-Qwen-7B-turbomind',
        path='open-r1/OpenR1-Qwen-7B',
        engine_config=dict(session_len=34768, max_batch_size=128, tp=1),
        gen_config=dict(
                        do_sample=True,
                        temperature=0.6,
                        top_p=0.95,
                        max_new_tokens=32768),
        max_seq_len=34768,
        max_out_len=32768,
        batch_size=128,
        run_cfg=dict(num_gpus=1),
    ),
    dict(
        type=TurboMindModelwithChatTemplate,
        abbr='open-thoughts--OpenThinker2-7B-turbomind',
        path='open-thoughts/OpenThinker2-7B',
        engine_config=dict(session_len=34768, max_batch_size=128, tp=1),
        gen_config=dict(
                        do_sample=True,
                        temperature=0.6,
                        top_p=0.95,
                        max_new_tokens=32768),
        max_seq_len=34768,
        max_out_len=32768,
        batch_size=128,
        run_cfg=dict(num_gpus=1),
    ),
    dict(
        type=TurboMindModelwithChatTemplate,
        abbr='Open-Reasoner-Zero--Open-Reasoner-Zero-7B-turbomind',
        path='Open-Reasoner-Zero/Open-Reasoner-Zero-7B',
        engine_config=dict(session_len=34768, max_batch_size=128, tp=1),
        gen_config=dict(
                        do_sample=True,
                        temperature=0.6,
                        top_p=0.95,
                        max_new_tokens=32768),
        max_seq_len=34768,
        max_out_len=32768,
        batch_size=128,
        run_cfg=dict(num_gpus=1),
    ),
    dict(
        type=TurboMindModelwithChatTemplate,
        abbr='hkust-nlp--Qwen-2.5-Math-7B-SimpleRL-Zoo-turbomind',
        path='hkust-nlp/Qwen-2.5-Math-7B-SimpleRL-Zoo',
        engine_config=dict(session_len=34768, max_batch_size=128, tp=1),
        gen_config=dict(
                        do_sample=False,
                        temperature=0.0,
                        top_p=1.0,
                        max_new_tokens=32768),
        max_seq_len=34768,
        max_out_len=32768,
        batch_size=128,
        run_cfg=dict(num_gpus=1),
    ),
    dict(
        type=TurboMindModelwithChatTemplate,
        abbr='Qwen--Qwen2.5-Math-7B-Instruct-turbomind',
        path='Qwen/Qwen2.5-Math-7B-Instruct',
        engine_config=dict(session_len=34768, max_batch_size=128, tp=1),
        gen_config=dict(
                        do_sample=False,
                        temperature=0.0,
                        top_p=1.0,
                        max_new_tokens=32768),
        max_seq_len=34768,
        max_out_len=32768,
        batch_size=128,
        run_cfg=dict(num_gpus=1),
    ),
    dict(
        type=TurboMindModelwithChatTemplate,
        abbr='Qwen--Qwen2.5-7B-Instruct-turbomind',
        path='Qwen/Qwen2.5-7B-Instruct',
        engine_config=dict(session_len=34768, max_batch_size=128, tp=1),
        gen_config=dict(
                        do_sample=False,
                        temperature=0.0,
                        top_p=1.0,
                        max_new_tokens=32768),
        max_seq_len=34768,
        max_out_len=32768,
        batch_size=128,
        run_cfg=dict(num_gpus=1),
    ),
    dict(
        type=TurboMindModelwithChatTemplate,
        abbr='PRIME-RL--Eurus-2-7B-PRIME-turbomind',
        path='PRIME-RL/Eurus-2-7B-PRIME',
        engine_config=dict(session_len=34768, max_batch_size=128, tp=1),
        gen_config=dict(
                        do_sample=True,
                        temperature=0.6,
                        top_p=0.95,
                        max_new_tokens=32768),
        max_seq_len=34768,
        max_out_len=32768,
        batch_size=128,
        run_cfg=dict(num_gpus=1),
    ),
    dict(
        type=TurboMindModelwithChatTemplate,
        abbr='AIDC-AI--Marco-o1-turbomind',
        path='AIDC-AI/Marco-o1',
        engine_config=dict(session_len=34768, max_batch_size=128, tp=1),
        gen_config=dict(
                        do_sample=False,
                        temperature=0.0,
                        top_p=1.0,
                        max_new_tokens=32768),
        max_seq_len=34768,
        max_out_len=32768,
        batch_size=128,
        run_cfg=dict(num_gpus=1),
    ),
]
